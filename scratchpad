




def appliance_anomaly_result(test_day,area_stat,device): # area_stat stores training models results
    """ function called by localize_anomalous_appliance,
    used at testing time """
    #print "start:"+device+":"+test_day.index.date[0].strftime('%d/%m/%Y')
    test_res = cluster_appliance_testing_stage(test_day)
    test_res = test_res.sort_values(by='mean_mag',ascending=True)
    test_res = test_res.reset_index(drop=True)
    area_stat = area_stat[device]
    for i in range(area_stat.shape[0]):
        if(test_res.loc[i].mean_area <= area_stat.loc[i].mean_area - 1.5 * area_stat.loc[i].sd_area):
            print device + "Frequent Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y')
        elif(test_res.loc[i].mean_area >= area_stat.loc[i].mean_area + 1.5 * area_stat.loc[i].sd_area):
            print device + "Elongated Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y')


def appliance_anomaly_result_version2(test_day,area_stat): # area_stat stores training models results
    """ function called by localize_anomalous_appliance,
    used at testing time """
    #print "start:"+device+":"+test_day.index.date[0].strftime('%d/%m/%Y')
    test_res = cluster_appliance_testing_stage(test_day)
    test_res = test_res.sort_values(by='mean_mag',ascending=True)
    test_res = test_res.reset_index(drop=True)
    #area_stat = area_stat
    for i in range(area_stat.shape[0]):
        if(test_res.loc[i].mean_area <= area_stat.loc[i].mean_area - 1.5 * area_stat.loc[i].sd_area):
            print "Frequent Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y')
        elif(test_res.loc[i].mean_area >= area_stat.loc[i].mean_area + 1.5 * area_stat.loc[i].sd_area):
            print "Elongated Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y')
 
            
def cluster_appliance_testing_stage_with_time_context(test_dat):
    
    """runs over the appliances outtput from NILM stage
    test_dat: is time_series power consumption data
    """
    from IPython import 
    hour = test_dat.index.hour
    selector = ((hour >= 0) & (hour <= 6)) | (hour>=18)
    night_data = test_dat[selector]
    day_data = test_dat.between_time('6:00','18:00')
    data_contexts = {'day':day_data,'night':night_data}
    context_result = {}
    for key,value in data_contexts.iteritems():
        data_arr = value.values
        temp = pd.DataFrame(data_arr)
        kmeans_ob =  KMeans(n_clusters=2).fit(data_arr.reshape(-1,1))
        temp['cluster'] = kmeans_ob.labels_
        temp.columns = ['consump','cluster']
        rle_vector = [(k,sum(1 for i in g)) for k,g in groupby(temp['cluster'])]
        rle_df =  pd.DataFrame(rle_vector,columns=["value","count"])
        unique_labels = np.repeat(range(rle_df.shape[0]),rle_df['count'])
        temp['unique_labels'] = unique_labels
        temp.index = value.index
        df_pd = pd.DataFrame(columns=['cluster','magnitude','duration','area'])
        for i in range(np.unique(unique_labels).size):
            temp_obj =  temp[unique_labels==i]    
            start_entry = temp_obj.head(1).index
            last_entry = temp_obj.tail(1).index
            duration_mins = ((last_entry-start_entry).total_seconds()/60.)[0]
            mean_usage = round(temp_obj['consump'].mean(),2)
            area_val = np.trapz(y=temp_obj['consump'])
            df_pd.loc[i] =  [np.unique(temp_obj['cluster'])[0],mean_usage,duration_mins,area_val]
        context_result[key] = compute_area_res_statistic(df_pd)
    return(context_result)