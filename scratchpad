




def appliance_anomaly_result(test_day,area_stat,device): # area_stat stores training models results
    """ function called by localize_anomalous_appliance,
    used at testing time """
    #print "start:"+device+":"+test_day.index.date[0].strftime('%d/%m/%Y')
    #test_res = cluster_appliance_testing_stage(test_day)
    test_res = cluster_appliance_testing_stage_with_time_context(test_day)
    #from IPython import embed
    #embed()
    #test_res = test_res.sort_values(by='mean_mag',ascending=True)
    #test_res = test_res.reset_index(drop=True)
    area_stat = area_stat[device]
    #contexts = area_stat.keys()
    for key,context in area_stat.iteritems():
        test = test_res[key] # area_stat -> context,test_res - > test
        for i in range(context.shape[0]): # for no. of rows corresponding to no.of clusters/states
            if(test.loc[i].mean_area <= context.loc[i].mean_area - 1.5 * context.loc[i].sd_area):
                print device + " Frequent Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y') +" at "+ key + " time"
            elif(test.loc[i].mean_area >= context.loc[i].mean_area + 1.5 * context.loc[i].sd_area):
                print device + " Elongated Anomaly on " + np.unique(test_day.index.date)[0].strftime('%d/%m/%Y') +" at "+ key + " time"   





def cluster_appliance_testing_stage_with_time_context(test_dat):
    
    """runs over the appliances outtput from NILM stage
    test_dat: is time_series power consumption data
    """
    #from IPython import embed
    #embed()
    hour = test_dat.index.hour
    selector = ((hour >= 0) & (hour <= 6)) | (hour>=18)
    night_data = test_dat[selector]
    day_data = test_dat.between_time('6:00','18:00')
    data_contexts = {'day':day_data,'night':night_data}
    context_result = {}
    for key,value in data_contexts.iteritems():
        data_arr = value.values
        temp = pd.DataFrame(data_arr)
        kmeans_ob =  KMeans(n_clusters=2).fit(data_arr.reshape(-1,1))
        temp['cluster'] = kmeans_ob.labels_
        temp.columns = ['consump','cluster']
        rle_vector = [(k,sum(1 for i in g)) for k,g in groupby(temp['cluster'])]
        rle_df =  pd.DataFrame(rle_vector,columns=["value","count"])
        unique_labels = np.repeat(range(rle_df.shape[0]),rle_df['count'])
        temp['unique_labels'] = unique_labels
        temp.index = value.index
        df_pd = pd.DataFrame(columns=['cluster','magnitude','duration','area'])
        for i in range(np.unique(unique_labels).size):
            temp_obj =  temp[unique_labels==i]    
            start_entry = temp_obj.head(1).index
            last_entry = temp_obj.tail(1).index
            duration_mins = ((last_entry-start_entry).total_seconds()/60.)[0]
            mean_usage = round(temp_obj['consump'].mean(),2)
            area_val = np.trapz(y=temp_obj['consump'])
            df_pd.loc[i] =  [np.unique(temp_obj['cluster'])[0],mean_usage,duration_mins,area_val]
        temp_res = compute_area_res_statistic(df_pd)
        temp_res = temp_res.sort_values(by='mean_mag',ascending=True)
        temp_res = temp_res.reset_index(drop=True)
        context_result[key] = temp_res
    return(context_result)



